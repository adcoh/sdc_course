{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COLAB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    # let's install some libraries\n",
    "    !pip install catboost plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    # workaround for making plotly be able to render pointclouds in colab\n",
    "    import IPython\n",
    "    def configure_plotly_browser_state():\n",
    "      display(IPython.core.display.HTML('''\n",
    "            <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "            <script>\n",
    "              requirejs.config({\n",
    "                paths: {\n",
    "                  base: '/static/base',\n",
    "                  plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
    "                },\n",
    "              });\n",
    "            </script>\n",
    "            '''))\n",
    "\n",
    "    IPython.get_ipython().events.register('pre_run_cell', configure_plotly_browser_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data downloading ####\n",
    "\n",
    "(Alternative link for manual downloading only [https://yadi.sk/d/IL739VKe8HhZ7g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_archive(path_to_archive, dst_folder='.'):\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(path_to_archive, 'r') as z:\n",
    "      z.extractall(dst_folder)\n",
    "\n",
    "import os\n",
    "if not os.path.exists('snow.zip'):\n",
    "    !wget https://www.dropbox.com/s/68hlj2dikxfcs3s/snow.zip?dl=0 -O snow.zip\n",
    "    extract_zip_archive('snow.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "  # see tutorial at  https://medium.com/@ml_kid/how-to-save-our-model-to-google-drive-and-reuse-it-2c1028058cb2\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lidar data segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does lidar look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('Pa-q5elS_nE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we obtain from lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "import tqdm\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "EQUAL_ASPECT_RATIO_LAYOUT = dict(\n",
    "    margin={\n",
    "        'l': 0,\n",
    "        'r': 0,\n",
    "        'b': 0,\n",
    "        't': 0\n",
    "    }, scene=dict(\n",
    "    aspectmode='data'\n",
    "))\n",
    "\n",
    "\n",
    "def color(x, cmap='Reds'):\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    x = (x - np.min(x)) / np.max(x)\n",
    "    \n",
    "    return cmap(x)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('snow.csv')\n",
    "ds = ds.set_index(['scene_id'])\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`snow.csv` contains labeled points from multiple scenes, so `scene_id` defines an identification of scene from which this point came. Let's check what are `intensity` and `ring`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://eckop.com/wp-content/uploads/2018/07/LIDAR_02-1024x366.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's color points according to their ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = ds.loc[0]\n",
    "\n",
    "fig = go.Figure(layout=EQUAL_ASPECT_RATIO_LAYOUT)\n",
    "fig.add_scatter3d(**{\n",
    "    'x': scene.x,\n",
    "    'y': scene.y,\n",
    "    'z': scene.z,\n",
    "    'mode': 'markers',\n",
    "    'marker': {\n",
    "        'size': 1,\n",
    "        'color': color(scene.ring, 'tab20'),\n",
    "    },\n",
    "    'text': scene.ring\n",
    "})\n",
    "\n",
    "py.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity\n",
    "\n",
    "Intensity represents a strength of reflected signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://res.mdpi.com/sensors/sensors-15-28099/article_deploy/html/images/sensors-15-28099-g001-1024.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=EQUAL_ASPECT_RATIO_LAYOUT)\n",
    "fig.add_scatter3d(**{\n",
    "    'x': scene.x,\n",
    "    'y': scene.y,\n",
    "    'z': scene.z,\n",
    "    'mode': 'markers',\n",
    "    'marker': {\n",
    "        'size': 1,\n",
    "        'color': color(scene.intensity, 'seismic'),\n",
    "    },\n",
    "    'text': scene.intensity\n",
    "})\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = ds.loc[1]\n",
    "\n",
    "fig = go.Figure(layout=EQUAL_ASPECT_RATIO_LAYOUT)\n",
    "fig.add_scatter3d(**{\n",
    "    'x': scene.x,\n",
    "    'y': scene.y,\n",
    "    'z': scene.z,\n",
    "    'mode': 'markers',\n",
    "    'marker': {\n",
    "        'size': 1,\n",
    "        'color': color(scene.label, 'seismic'),\n",
    "    },\n",
    "    'text': scene.label\n",
    "})\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1 - heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_intensity_points_mask(intensity, limit=3):\n",
    "    return intensity > limit\n",
    "\n",
    "filtered_scene = scene[get_high_intensity_points_mask(scene.intensity)]\n",
    "\n",
    "fig = go.Figure(layout=EQUAL_ASPECT_RATIO_LAYOUT)\n",
    "fig.add_scatter3d(**{\n",
    "    'x': filtered_scene.x,\n",
    "    'y': filtered_scene.y,\n",
    "    'z': filtered_scene.z,\n",
    "    'mode': 'markers',\n",
    "    'marker': {\n",
    "        'size': 1,\n",
    "        'color': color(filtered_scene.intensity, 'seismic'),\n",
    "    },\n",
    "    'text': filtered_scene.intensity\n",
    "})\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but some snow points are still be in pointcloud while some obstacle points were filtered.\n",
    "\n",
    "Can we improve the baseline? Yes! Let's train catboost (or lightgbm / GBDT / Random Forest / Neural network / whatsoever) to segment pointcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2 - catboost on hand-crafted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "The previous baseline model processed each point independently and used just point intesity as a feature.\n",
    "Let's use more features and process each point with its neigbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "class FeaturesExtractor(object):\n",
    "    def __init__(self, r=1.0):\n",
    "        self.xyz = None\n",
    "        self.intensity = None\n",
    "        self.ring = None\n",
    "        self.index = None\n",
    "        self.r = r\n",
    "\n",
    "    def _feature_names(self):\n",
    "        \"\"\"\n",
    "        Returns a list of feature names\n",
    "        \"\"\"\n",
    "        names = [\n",
    "            'n_points', \n",
    "            'min_intensity', 'max_intensity', 'median_intensity', 'std_intensity',\n",
    "            'min_ring', 'max_ring', 'median_ring', 'std_ring'\n",
    "        ]\n",
    "        return ['x', 'y', 'z', 'intensity', 'ring'] + \\\n",
    "                ['{}_{}'.format(name, self.r) for name in names]\n",
    "\n",
    "    def compute_point_features(self, point_id, neighbours):\n",
    "        \"\"\"\n",
    "        Returns a list of features for given neighbours\n",
    "        'neighbours' - list of indices\n",
    "        \"\"\"\n",
    "        # TODO implement it\n",
    "        \n",
    "        # NOTE if you want to work with point coordinates here, \n",
    "        # it is a good idea to normalize these coordinates\n",
    "        # to have zero at point of interest:\n",
    "        #neighbour_points_xyz = self.xyz[neighbours]\n",
    "        #point_xyz = self.xyz[point_id]\n",
    "        #neighbour_points_xyz = neighbour_points_xyz - point_xyz\n",
    "        x, y, z = self.xyz[point_id]\n",
    "        intensity = self.intensity[point_id]\n",
    "        ring = self.ring[point_id]\n",
    "        nn_intensity = self.intensity[neighbours]\n",
    "        nn_ring = self.ring[neighbours]\n",
    "        features = [\n",
    "            x,y,z,intensity, ring,\n",
    "            len(neighbours),\n",
    "            np.min(nn_intensity), np.max(nn_intensity), np.median(nn_intensity), np.std(nn_intensity),\n",
    "            np.min(nn_ring), np.max(nn_ring), np.median(nn_ring), np.std(nn_ring)\n",
    "        ]\n",
    "        return features\n",
    "\n",
    "    def get_point_neighbours(self, point_id):\n",
    "        \"\"\"\n",
    "        Returns a list of indices of neighbour points\n",
    "        \"\"\"\n",
    "        return self.index.query_radius(self.xyz[point_id][np.newaxis, :], r=self.r)[0]\n",
    "\n",
    "    def __call__(self, xyz, intensity, ring):\n",
    "        self.xyz = xyz[:]\n",
    "        self.intensity = intensity[:]\n",
    "        self.ring = ring[:]\n",
    "        \n",
    "        self.index = KDTree(self.xyz)\n",
    "                \n",
    "        features = []\n",
    "        for point_id in range(len(self.xyz)):\n",
    "            neighbours = self.get_point_neighbours(point_id)\n",
    "            features.append(self.compute_point_features(point_id, neighbours))\n",
    "        return pd.DataFrame(columns=self._feature_names(), data=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# FIXME\n",
    "FEATURES_FOLDER = 'features' if not USE_COLAB else \"/content/gdrive/My Drive/y_data_sdc_hw2_features\"\n",
    "if not os.path.exists(FEATURES_FOLDER):\n",
    "    os.mkdir(FEATURES_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will take 20-50 minutes for computing features. You can either compute features from scratch or reuse precomputed features from file `./snow_features.csv` (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_extractor = FeaturesExtractor(r=1.0)\n",
    "\n",
    "for scene_id in tqdm.tqdm(ds.reset_index().scene_id.unique()):\n",
    "    scene = ds.loc[scene_id]\n",
    "    features_df = \\\n",
    "        features_extractor(scene[['x', 'y', 'z']].values, scene.intensity.values, scene.ring.values)\n",
    "    features_df['label'] = scene.label.values\n",
    "    features_df.to_csv(os.path.join(FEATURES_FOLDER, '{}.csv'.format(scene_id)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_features = []\n",
    "for scene in os.listdir(FEATURES_FOLDER):\n",
    "    scene_features = pd.read_csv(os.path.join(FEATURES_FOLDER, scene))\n",
    "    scene_id = int(os.path.splitext(scene)[0])\n",
    "    scene_features['scene_id'] = scene_id\n",
    "    ds_features.append(scene_features)\n",
    "    \n",
    "ds_features = pd.concat(ds_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_features.to_csv('./snow_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of computing features from scratch, you can load precomputed features from file `./snow_features.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_features = pd.read_csv('./snow_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/val/test split\n",
    "How to split - by scenes or by points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_ids = ds_features.scene_id.unique()\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(scene_ids)\n",
    "n = len(scene_ids)\n",
    "train_ids = scene_ids[:int(0.8*n)]\n",
    "val_ids = scene_ids[int(0.8*n): -int(0.1*n)]\n",
    "test_ids = scene_ids[-int(0.1*n):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds_features[ds_features.scene_id.isin(train_ids)]\n",
    "test = ds_features[ds_features.scene_id.isin(test_ids)]\n",
    "val = ds_features[ds_features.scene_id.isin(val_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "\n",
    "def learn(X_train, X_val, y_train, y_val):\n",
    "    clf = catboost.CatBoostClassifier(n_estimators=100)\n",
    "    clf.fit(\n",
    "        X_train, y_train, early_stopping_rounds=10,\n",
    "        use_best_model=True, eval_set=(X_val.values, y_val.values), plot=True, verbose=False)\n",
    "    return clf\n",
    "\n",
    "X_train = train.drop([\"scene_id\", \"label\"], axis=1)\n",
    "y_train = train.label\n",
    "\n",
    "\n",
    "X_val = val.drop([\"scene_id\", \"label\"], axis=1)\n",
    "y_val = val.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = learn(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(['scene_id', 'label'], axis=1)\n",
    "y_test = test.label\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "\n",
    "def test_one(clf, X_test, y_test):\n",
    "    y_test_hat = clf.predict_proba(X_test)\n",
    "    pr, rec, thr = precision_recall_curve(y_test, y_test_hat[:, 1])\n",
    "    ix = np.linspace(1, len(pr)-1, num=2000).astype(int)\n",
    "    return pr[ix], rec[ix], thr[ix - 1]\n",
    "\n",
    "\n",
    "def heuristic_filter_scoring():\n",
    "    pr = []\n",
    "    rec = []\n",
    "    filter_range = list(range(1, 10))\n",
    "    for i in filter_range:\n",
    "        y_test_heuristic_hat = np.ones(len(X_test))\n",
    "        y_test_heuristic_hat[get_high_intensity_points_mask(test.intensity, i)] = 0\n",
    "        pr.append(precision_score(y_test, y_test_heuristic_hat))\n",
    "        rec.append(recall_score(y_test, y_test_heuristic_hat))\n",
    "        \n",
    "    return pr, rec, filter_range\n",
    "\n",
    "pr_bl, rec_bl, thr_bl = heuristic_filter_scoring()\n",
    "\n",
    "def plot_pr_rec(*models):\n",
    "    traces = []\n",
    "    for model, clf, X_test, y_test in models:\n",
    "        pr, rec, thr = test_one(clf, X_test, y_test)\n",
    "        pr_rec = go.Scattergl(x = rec, y = pr, mode='lines', text=thr, name=model)\n",
    "        traces.append(pr_rec)\n",
    "\n",
    "    pr_rec_bl = go.Scatter(x = rec_bl, y = pr_bl, mode='lines+markers', text=thr_bl, name='Intensity BL')\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Precission-recall',\n",
    "        xaxis=dict(\n",
    "            title='Recall'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Precission'\n",
    "        ))\n",
    "    fig = go.Figure(\n",
    "        data=traces + [pr_rec_bl],\n",
    "        layout=layout)\n",
    "    py.iplot(fig)\n",
    "    \n",
    "models = [('Catboost classifier', cls, X_test, y_test)]\n",
    "plot_pr_rec(*models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = cls.predict_proba(test.drop(['scene_id', 'label'], axis=1))[:, 1]  # confidence for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = test_ids[0]\n",
    "scene = ds.loc[scene_id]\n",
    "scene_predictions = y_test_hat[test.scene_id == scene_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=EQUAL_ASPECT_RATIO_LAYOUT)\n",
    "fig.add_scatter3d(**{\n",
    "    'x': scene.x,\n",
    "    'y': scene.y,\n",
    "    'z': scene.z,\n",
    "    'mode': 'markers',\n",
    "    'marker': {\n",
    "        'size': 1,\n",
    "        'color': color(scene_predictions, 'seismic'),\n",
    "    },\n",
    "    'text': scene_predictions\n",
    "})\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to train a better model that detects snow points and **beats the quality of baseline 2**. Target metric: precision-at-recall 0.95 (see implementation below).\n",
    "\n",
    "What you **can** do:\n",
    "* Train pointnet-like neural network to classify point\n",
    "* Play with features extraction. For example, we didn't use neighbour point coordinates, just intensities and rings.\n",
    "\n",
    "What you **can't** do:\n",
    "* Just tune catboost (or any other off-the-shelf machine learning model) parameters while keeping the same features as in baseline 2. Please, do some lidar-related experiments.\n",
    "\n",
    "As a solution, please send \n",
    "1) ipython notebook (or python script) with training code and \n",
    "2) csv file with results on hold-out dataset: https://www.dropbox.com/s/n8zy85i0uzsinqa/snow_test_X.csv .\n",
    "Output csv file should contain two columns: `point_id` (from input file) and `confidence` with confidence from your detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_at_recall(y_pred, y_true, minimal_recall=0.95):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      y_pred - np.array of shape (N,) with prediction confidences\n",
    "      y_true - ground truth labels (0 or 1), np.array of shape (N,)\n",
    "    \"\"\"\n",
    "    positive_points_mask = y_true == 1\n",
    "    positive_points_conf = y_pred[positive_points_mask]\n",
    "    n_positive_points = len(positive_points_conf)\n",
    "    n_positive_points_to_skip = int((1. - minimal_recall) * n_positive_points)\n",
    "    threshold = np.partition(positive_points_conf, kth=n_positive_points_to_skip)[n_positive_points_to_skip]\n",
    "    accepted_points_mask = y_pred >= threshold\n",
    "    return np.mean(y_true[accepted_points_mask].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (calculate_precision_at_recall(y_test_hat, test.label.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(output_file, point_ids, y_pred):\n",
    "    assert len(point_ids) == len(y_pred)\n",
    "    df = pd.DataFrame(columns=[\"point_id\", 'confidence'], data=list(zip(point_ids, y_pred)))\n",
    "    df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
